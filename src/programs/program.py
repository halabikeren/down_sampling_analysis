import os
import typing as t
from dataclasses import dataclass
from datetime import datetime
from time import time

import re

from utils import Job
import json

from dotenv import load_dotenv, find_dotenv

load_dotenv(find_dotenv())

import logging

logger = logging.getLogger(__name__)


@dataclass
class Program:
    name: str
    program_exe: str
    cluster_program_exe: str
    input_param_name: str = ""
    output_param_name: str = ""  # maps parameter name to parameter value

    def __init__(
        self):  # set to allow inheriting classes to receive additional arguments for setting additional
        pass

    @staticmethod
    def set_additional_params(
        additional_params: t.Dict[str, str],
        parallelize: bool,
        cluster_data_dir: str,
        return_as_str: bool = True,
    ) -> t.Optional[str]:
        """
        :param additional_params: dictionary mapping names to values f additional parameters for the program
        :param parallelize: indicates weather the execution
        of the program should be parallelized. In such case, any additional parameter whose value is a directory will
        be translated ot an absolute cluster directory
        :param cluster_data_dir the cluster data directory that should convert the container or relative directories of
        paths to absolute ones in the cluster
        :param return_as_str: indicates weather the additional parameters should be returned as a string
        :return: if the addition of parameters is expressed via the command line, a string that should be added to it
        will be returned
        """
        for field in additional_params:
            if "/" in additional_params[field]:
                additional_params[field] = (
                    f"{os.environ['container_data_dir']}{additional_params[field]}"
                    if not parallelize
                    else f"{cluster_data_dir}{additional_params[field]}"
                )
            additional_params_str = " ".join(
                [
                    f"{param_name} {additional_params[param_name]}"
                    for param_name in additional_params
                ]
            )
            if return_as_str:
                return additional_params_str

    def set_command(
        self,
        input_path: str,
        output_path: str,
        additional_params: t.Optional[t.Dict[str, str]],
        parallelize: bool,
        cluster_data_dir: str,
        **kwargs,
    ) -> str:
        """
        :param input_path: path to the input of the program
        :param output_path: path to the output of the program
        :param additional_params: additional parameters to run the program with (maps parameter name to value)
        :param parallelize: boolean indicating weather program execution should be parallelized
        :param cluster_data_dir: directory to concat to directory arguments in case of parallelization on the cluster
        :return: a string representing the command
        """
        program_input_path = (
            input_path
            if not parallelize
            else input_path.replace(os.environ["container_data_dir"], cluster_data_dir)
        )
        program_output_path = (
            output_path
            if not parallelize
            else output_path.replace(os.environ["container_data_dir"], cluster_data_dir)
        )
        input_str = f"{self.input_param_name} {program_input_path}"
        output_str = f"{self.output_param_name} {program_output_path}"
        additional_params_str = ""
        if additional_params:
            self.set_additional_params(additional_params, additional_params_str, cluster_data_dir)
        command = f"{self.program_exe if not parallelize else self.cluster_program_exe} {input_str} {output_str} {additional_params_str} "
        return command


    def exec(
        self,
        input_path: str,
        output_path: str,
        aux_dir: str,
        additional_params: t.Optional[t.Dict[str, str]] = None,
        parallelize: bool = False,
        cluster_data_dir: t.Optional[str] = None,
        priority: int = 0,
        queue: str = "itaym",
        wait_until_complete: bool = False,
        get_completion_validator: bool = True,
        **kwargs
    ) -> t.Union[float, str]:
        """
        :param input_path: path to alignment file
        :param output_path: path in which the program should write its output
        :param additional_params: additional parameters unique to the program
        :param parallelize: boolean indicating weather execution of the program should be parallelized in the cluster or not
        :param cluster_data_dir: cluster directory that is mounted to the container data directory. Must be provided with parallleize is True
        :param aux_dir: directory in which auxiliary files should be generated by the job submission process
        :param priority: priority of the jobs
        :param queue: queue to submit the jobs to
        :param wait_until_complete: indicator weather the main program should wait until completion of all jobs (recommended: True)
        :param get_completion_validator: boolean indicating weather a validator file should be generated upon job completion (recommended: True)
        :return: either the duration of the command, if no parallelization was selected, or the path to the touch file that is used for validation of job completion in case of parallelization
        """
        command = self.set_command(
            input_path=input_path, output_path=output_path, additional_params=additional_params, parallelize=parallelize, cluster_data_dir=cluster_data_dir, **kwargs)
        os.makedirs(aux_dir, exist_ok=True)

        if os.path.exists(output_path):
            logger.info(f"{self.name} output already exists at {output_path} and will not be generated again")
            return

        if not parallelize:
            start_time = time()
            os.chdir(aux_dir)  # move to aux dir as rate4site generates extra files in current running directory
            res = os.system(f"{command} > /dev/null 2>&1")  # for some reason, rate4 prints some logs into the stderr,
            # making the typical test (raise error i=f stderr > 0) invalid in this case
            if res != 0:
                raise RuntimeError(
                    f"command {command} failed to execute."
                )
            end_time = time()
            return end_time - start_time
        else:
            commands = [
                f"cd {aux_dir.replace(os.environ['container_data_dir'], cluster_data_dir)}",
                '''timestamp() {
                      date +"%T" # current time
                    }
                    timestamp''',
                command,
                'timestamp',
            ]

            job = Job(
                name=self.name,
                sh_dir=aux_dir,
                output_dir=aux_dir,
                commands=commands,
                priority=priority,
                queue=queue,
            )
            completion_validator = job.submit(
                wait_until_complete=wait_until_complete,
                get_completion_validator=get_completion_validator,
            )
            return completion_validator

    @staticmethod
    def parse_output(output_path: str, job_output_dir: t.Optional[str] = None) -> t.Dict[str, t.Any]:
        """
        :param output_path: path holding the output of the program
        :param job_output_dir: directory holding the output of the job, in case parallelization was chosen
        :return: a dictionary holding the parsed result of the program execution
        """
        if not os.path.exists(output_path):
            raise ValueError(f"output path {output_path} does not exist")
        with open(output_path, "r") as out:
            output_content = out.read()

        result = {"row_output": output_content}

        if job_output_dir:
            timestamp_regex = re.compile("(\d*\:\d*\:\d*)")
            job_output_path = [path for path in os.listdir(job_output_dir) if ".OU" in path][0]
            with open(job_output_path, "r") as job_output_file:
                content = job_output_file.read()
            times = [match.group(1) for match in timestamp_regex.finditer(content)]
            start_time = datetime.strptime(times[0], "%H:%M:%S")
            end_time = datetime.strptime(times[-1], "%H:%M:%S")
            duration = (end_time-start_time).total_seconds()/60
            result["duration(minutes)"] = duration

        return result

    @staticmethod
    def write_result_json(input_path: str, job_output_dir: t.Optional[str], output_path: str):
        """
        :param input_path: path to file with the raw program results
        :param job_output_dir: path to the job's output which holds duration measures in case of parallelization
        :param output_path: path ot write the json file to
        :return:
        """
        output_dir = os.path.dirname(output_path)
        os.makedirs(output_dir, exist_ok=True)
        result = Program.parse_output(input_path=input_path, job_output_dir=job_output_dir)
        with open(output_path, "w") as output:
            json.dump(result, output)

